{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5cdb63",
   "metadata": {},
   "source": [
    "## Create Training and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5af4f",
   "metadata": {},
   "source": [
    "### Importing Dependencies\n",
    "\n",
    "We import the necessary libraries and functions, ensuring that all required modules and helper functions are properly integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514b8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import sys\n",
    "import torch\n",
    "import import_ipynb \n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from utils.wrapper.transform_networkx_into_pyg import transform_networkx_into_pyg\n",
    "from utils.helper_functions.add_dummy_node_features import add_dummy_node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde60ae",
   "metadata": {},
   "source": [
    "### Loading and Preparing Graph Data from GraphML Files\n",
    "\n",
    "The `load_graphml_files` function loads a series of bicycle traffic network graphs stored in GraphML format and prepares them for training with PyTorch Geometric (PyG). The objective is to convert each monthly graph into a format compatible with Graph Neural Networks (GNNs), ensuring that both node and edge features are retained.\n",
    "\n",
    "Each NetworkX graph is converted into a PyG `Data` object using a custom helper function `transform_networkx_into_pyg`. This function ensures that essential node and edge attributes such as:\n",
    "\n",
    "- **Node attributes**:\n",
    "  - `lon` (longitude),\n",
    "  - `lat` (latitude),\n",
    "  \n",
    "- **Edge attributes**:\n",
    "  - `speed_rel` (relative speed),\n",
    "  - `month` (for cyclical encoding of months),\n",
    "  - `year` (the year of the traffic data),\n",
    "  - `id` (a unique identifier for each edge),\n",
    "  - `tracks` (the number of bicycles traveling from the starting to the ending point),\n",
    "\n",
    "are preserved during the conversion process.\n",
    "\n",
    "PyG expects data in a specific structure, particularly when both node and edge attributes are used in models like GATv2.\n",
    "\n",
    "`data_list` contains multiple `torch_geometric.data.Data` objects, each representing a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d326e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphml_files(years=[2021, 2022, 2023]):\n",
    "    \"\"\"\n",
    "    Loads multiple directed graph files in GraphML format and converts them \n",
    "    into PyTorch Geometric (PyG) Data objects.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    years : list of int, optional (default=[2021, 2022, 2023])\n",
    "        List of years for which graph files should be loaded. \n",
    "        Assumes 12 monthly files per year.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_list : list of torch_geometric.data.Data\n",
    "        List of PyG data objects created from the loaded NetworkX graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for year in years:\n",
    "        for i in range(12):\n",
    "            path = f\"../../../data/graphml/{year}/bike_network_{year}_{i}.graphml\"\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"[WARN] File not found: {path}\")\n",
    "                continue\n",
    "\n",
    "            G_nx = nx.read_graphml(path)\n",
    "            G_nx = nx.DiGraph(G_nx)\n",
    "\n",
    "            data = transform_networkx_into_pyg(G_nx)\n",
    "            data_list.append(data)\n",
    "\n",
    "    print(f\"Number of loaded graphs: {len(data_list)}\")\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896be71",
   "metadata": {},
   "source": [
    "### Adding Dummy Node Features to the Graphs\n",
    "\n",
    "In this section of the code, we add **dummy node features** to our graphs. This process ensures that each node in our graphs has a **feature dimension**, even if no node features were originally present. This is an important step in preparing the data for use in Graph Neural Networks (GNNs).\n",
    "\n",
    "**NOTE:** At a later stage, once we have implemented feature engineering, we will replace the dummy features with the engineered ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3550c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data_list, feature_dim=1, value=1.0):\n",
    "    # add dummy feature\n",
    "    return add_dummy_node_features(data_list, feature_dim=feature_dim, value=value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c609a6",
   "metadata": {},
   "source": [
    "### Train-Validation Split\n",
    "\n",
    "For predicting edge attributes (e.g., `tracks`), an 80/20 train/validation split is applied to the **existing edges within each graph**.\n",
    "\n",
    "In our application, the **nodes represent physically existing bike stations**, which typically do not change or only change very infrequently. The aim of the analysis is to model the **connections between stations**, i.e., to understand and predict how many bicycles move along certain routes (in other words: edges with weights).\n",
    "\n",
    "A **node-level split** (i.e., an 80/20 split of the nodes themselves) would mean that some stations would be completely unseen during training. This would not be meaningful because:\n",
    "\n",
    "- The **stations themselves are not the prediction target**;\n",
    "- It is the **relationships or transitions between the stations (edges)** that should be modeled;\n",
    "- In deployment, **all stations are known** (they are physically installed in the system);\n",
    "\n",
    "Initially, we wanted to use the `RandomLinkSplit()` function, but this is designed for classic link prediction â€“ i.e., binary classification. It adds both positive examples (existing edges) AND negative examples (non-existing edges). Since our task is an edge attribute regression task, this method is unsuitable, and we manually implemented the split mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8119f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(data_list, val_ratio=0.2, seed=42, save_dir=\"../../../data/data_splits\", edge_attr_key_index=4):\n",
    "    \"\"\"\n",
    "    Splits a list of PyTorch Geometric Data objects into training and validation sets\n",
    "    for edge regression tasks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_list : list of torch_geometric.data.Data\n",
    "        List of graphs to be split into train and validation sets.\n",
    "    \n",
    "    val_ratio : float, optional (default=0.2)\n",
    "        Proportion of edges to be used for validation in each graph.\n",
    "    \n",
    "    seed : int, optional (default=42)\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    save_dir : str, optional (default='../../../data/data_splits')\n",
    "        Directory path where the split datasets will be saved.\n",
    "\n",
    "    edge_attr_key_index : int, optional (default=4)\n",
    "        The index of the edge attribute that should be predicted (e.g. 'tracks').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_save_path = os.path.join(save_dir, \"train_data.pt\")\n",
    "    val_save_path = os.path.join(save_dir, \"val_data.pt\")\n",
    "\n",
    "    train_list, val_list = [], []\n",
    "    total_train_edges = 0\n",
    "    total_val_edges = 0\n",
    "\n",
    "    for i, data in enumerate(data_list):\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        num_edges = edge_index.size(1)\n",
    "        num_val = int(val_ratio * num_edges)\n",
    "        perm = torch.randperm(num_edges)\n",
    "\n",
    "        val_idx = perm[:num_val]\n",
    "        train_idx = perm[num_val:]\n",
    "\n",
    "        # Training Data\n",
    "        train_data = data.clone()\n",
    "        train_data.edge_index = edge_index[:, train_idx]\n",
    "        train_data.edge_attr = torch.cat([edge_attr[train_idx][:, :edge_attr_key_index], edge_attr[train_idx][:, edge_attr_key_index+1:]], dim=1)\n",
    "        train_data.y = edge_attr[train_idx][:, edge_attr_key_index]  \n",
    "\n",
    "        # Validation Data\n",
    "        val_data = data.clone()\n",
    "        val_data.edge_index = edge_index[:, val_idx]\n",
    "        val_data.edge_attr = torch.cat([edge_attr[val_idx][:, :edge_attr_key_index], edge_attr[val_idx][:, edge_attr_key_index+1:]], dim=1)\n",
    "        val_data.y = edge_attr[val_idx][:, edge_attr_key_index]\n",
    "\n",
    "        train_list.append(train_data)\n",
    "        val_list.append(val_data)\n",
    "\n",
    "        total_train_edges += train_data.edge_index.size(1)\n",
    "        total_val_edges += val_data.edge_index.size(1)\n",
    "\n",
    "        print(f\"Graph {i}: Train edges = {train_data.edge_index.size(1)}, Val edges = {val_data.edge_index.size(1)}\")\n",
    "\n",
    "    # Batch the split data\n",
    "    train_data = Batch.from_data_list(train_list)\n",
    "    val_data = Batch.from_data_list(val_list)\n",
    "\n",
    "    # Save\n",
    "    torch.save(train_data, train_save_path)\n",
    "    torch.save(val_data, val_save_path)\n",
    "\n",
    "    print(f\"\\nTotal train edges (batched): {total_train_edges}\")\n",
    "    print(f\"Total val edges   (batched): {total_val_edges}\")\n",
    "    print(f\"\\nTrain data saved to: {train_save_path}\")\n",
    "    print(f\"Val data saved to: {val_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034c184",
   "metadata": {},
   "source": [
    "### Executing the Pipeline for Creating Training and Validation Data\n",
    "\n",
    "This script defines a `main` function that orchestrates the entire pipeline for generating training and validation splits for Graph Neural Networks (GNNs). The previously defined functions are called sequentially to load the graph data, perform feature engineering(not yet), and perform the data split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1707a1e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal data saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Call the main function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(years, save_dir, val_ratio)\u001b[39m\n\u001b[32m     23\u001b[39m val_save_path = os.path.join(save_dir, \u001b[33m\"\u001b[39m\u001b[33mval_data.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m data_list = \u001b[43mload_graphml_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43myears\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m train_data, val_data = split_train_val(data_list, val_ratio=val_ratio)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Normalize features\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mload_graphml_files\u001b[39m\u001b[34m(years)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[WARN] File not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m G_nx = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_graphml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m G_nx = nx.DiGraph(G_nx)\n\u001b[32m     30\u001b[39m data = transform_networkx_into_pyg(G_nx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 6:5\u001b[39m, in \u001b[36margmap_read_graphml_1\u001b[39m\u001b[34m(path, node_type, edge_key_type, force_multigraph, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\networkx\\utils\\backends.py:967\u001b[39m, in \u001b[36m_dispatchable.__call__\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend != \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    966\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m backend is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[32m    972\u001b[39m backend_name = backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:297\u001b[39m, in \u001b[36mread_graphml\u001b[39m\u001b[34m(path, node_type, edge_key_type, force_multigraph)\u001b[39m\n\u001b[32m    295\u001b[39m reader = GraphMLReader(node_type, edge_key_type, force_multigraph)\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# need to check for multiple graphs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m glist = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(glist) == \u001b[32m0\u001b[39m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# If no graph comes back, try looking for an incomplete header\u001b[39;00m\n\u001b[32m    300\u001b[39m     header = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<graphml xmlns=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttp://graphml.graphdrawing.org/xmlns\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:860\u001b[39m, in \u001b[36mGraphMLReader.__call__\u001b[39m\u001b[34m(self, path, string)\u001b[39m\n\u001b[32m    858\u001b[39m (keys, defaults) = \u001b[38;5;28mself\u001b[39m.find_graphml_keys(\u001b[38;5;28mself\u001b[39m.xml)\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.xml.findall(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.NS_GRAPHML\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33mgraph\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:890\u001b[39m, in \u001b[36mGraphMLReader.make_graph\u001b[39m\u001b[34m(self, graph_xml, graphml_keys, defaults, G)\u001b[39m\n\u001b[32m    888\u001b[39m \u001b[38;5;66;03m# add edges\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m edge_xml \u001b[38;5;129;01min\u001b[39;00m graph_xml.findall(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.NS_GRAPHML\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33medge\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m890\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_xml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraphml_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[38;5;66;03m# add graph data\u001b[39;00m\n\u001b[32m    892\u001b[39m data = \u001b[38;5;28mself\u001b[39m.decode_data_elements(graphml_keys, graph_xml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:938\u001b[39m, in \u001b[36mGraphMLReader.add_edge\u001b[39m\u001b[34m(self, G, edge_element, graphml_keys)\u001b[39m\n\u001b[32m    936\u001b[39m source = \u001b[38;5;28mself\u001b[39m.node_type(edge_element.get(\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    937\u001b[39m target = \u001b[38;5;28mself\u001b[39m.node_type(edge_element.get(\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode_data_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphml_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_element\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# GraphML stores edge ids as an attribute\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# NetworkX uses them as keys in multigraphs too if no key\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;66;03m# attribute is specified\u001b[39;00m\n\u001b[32m    942\u001b[39m edge_id = edge_element.get(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:973\u001b[39m, in \u001b[36mGraphMLReader.decode_data_elements\u001b[39m\u001b[34m(self, graphml_keys, obj_xml)\u001b[39m\n\u001b[32m    971\u001b[39m text = data_element.text\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# assume anything with subelements is a yfiles extension\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_element\u001b[49m\u001b[43m)\u001b[49m) == \u001b[32m0\u001b[39m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_type == \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    975\u001b[39m         \u001b[38;5;66;03m# Ignore cases.\u001b[39;00m\n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m# http://docs.oracle.com/javase/6/docs/api/java/lang/\u001b[39;00m\n\u001b[32m    977\u001b[39m         \u001b[38;5;66;03m# Boolean.html#parseBoolean%28java.lang.String%29\u001b[39;00m\n\u001b[32m    978\u001b[39m         data[data_name] = \u001b[38;5;28mself\u001b[39m.convert_bool[text.lower()]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main(years=[2021, 2022, 2023], save_dir=\"../../../data/data_splits\", val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Main pipeline for loading graph data, preprocessing it, and splitting into train/val sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    years : list of int, optional (default=[2021, 2022, 2023])\n",
    "        The years for which GraphML files will be loaded.\n",
    "\n",
    "    save_dir : str, optional (default='../../../data/data_splits')\n",
    "        Directory where the processed train and validation data will be saved.\n",
    "\n",
    "    val_ratio : float, optional (default=0.2)\n",
    "        Proportion of edges to be used for validation during the train/validation split.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    train_save_path = os.path.join(save_dir, \"train_data.pt\")\n",
    "    val_save_path = os.path.join(save_dir, \"val_data.pt\")\n",
    "\n",
    "    # Load data\n",
    "    data_list = load_graphml_files(years)\n",
    "    train_data, val_data = split_train_val(data_list, val_ratio=val_ratio)\n",
    "\n",
    "    # Normalize features\n",
    "    train_data, val_data = normalize_feature(train_data, val_data)\n",
    "\n",
    "    # Save\n",
    "    torch.save(train_data, train_save_path)\n",
    "    torch.save(val_data, val_save_path)\n",
    "\n",
    "    print(f\"\\nTrain data saved to: {train_save_path}\")\n",
    "    print(f\"Val data saved to: {val_save_path}\")\n",
    "\n",
    "# Call the main function\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
