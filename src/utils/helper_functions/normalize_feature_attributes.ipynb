{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145e4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca70c5",
   "metadata": {},
   "source": [
    "ToDos\n",
    "\n",
    "- Mögliche Überlegung: id als string und dann label encoding\n",
    "Label Encoding nach der Umwandlung in Strings wird dazu führen, dass die IDs neu nummeriert werden, und dabei wird der Abstand zwischen den ursprünglichen IDs ignoriert. Solange das Modell keine Bedeutung aus den Abständen zwischen den IDs ableitet, gibt es keine negativen Auswirkungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7782bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(train_batch, val_batch, save_dir=None):\n",
    "    \"\"\"\n",
    "    Normalizes edge and node attributes in batched train/val graph data.\n",
    "    Applies cyclical encoding for 'month' in edges and uses StandardScaler for all other features.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### === Normalize targets (y) === ###\n",
    "    y_train = train_batch.y.view(-1, 1).numpy()\n",
    "    y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "    train_batch.y = torch.tensor(y_scaler.transform(train_batch.y.view(-1, 1)), dtype=torch.float32)\n",
    "    val_batch.y = torch.tensor(y_scaler.transform(val_batch.y.view(-1, 1)), dtype=torch.float32)\n",
    "\n",
    "    ### === Normalize edge attributes === ###\n",
    "    month_idx = 1\n",
    "    feat_indices = [0, 2]  # speed_rel and year\n",
    "\n",
    "    edge_feats_train = train_batch.edge_attr[:, feat_indices].numpy()\n",
    "    feat_scaler = StandardScaler().fit(edge_feats_train)\n",
    "\n",
    "    for batch in [train_batch, val_batch]:\n",
    "        feat_tensor = batch.edge_attr[:, feat_indices]\n",
    "        feat_scaled = torch.tensor(feat_scaler.transform(feat_tensor.numpy()), dtype=torch.float32)\n",
    "\n",
    "        # Cyclical encoding for month\n",
    "        month_raw = batch.edge_attr[:, month_idx]\n",
    "        month_sin = torch.sin(2 * np.pi * month_raw / 12).view(-1, 1)\n",
    "        month_cos = torch.cos(2 * np.pi * month_raw / 12).view(-1, 1)\n",
    "\n",
    "        batch.edge_attr = torch.cat([feat_scaled, month_sin, month_cos], dim=1)\n",
    "\n",
    "    ### === Normalize node features (lon, lat) === ###\n",
    "    node_feats_train = train_batch.x.numpy()\n",
    "    node_scaler = StandardScaler().fit(node_feats_train)\n",
    "\n",
    "    train_batch.x = torch.tensor(node_scaler.transform(train_batch.x.numpy()), dtype=torch.float32)\n",
    "    val_batch.x = torch.tensor(node_scaler.transform(val_batch.x.numpy()), dtype=torch.float32)\n",
    "\n",
    "    ### === Save scalers === ###\n",
    "    if save_dir:\n",
    "        scaler_dir = os.path.join(save_dir, \"scalers\")\n",
    "        os.makedirs(scaler_dir, exist_ok=True)\n",
    "        joblib.dump(y_scaler, os.path.join(scaler_dir, \"target_scaler.pkl\"))\n",
    "        joblib.dump(feat_scaler, os.path.join(scaler_dir, \"edge_scaler.pkl\"))\n",
    "        joblib.dump(node_scaler, os.path.join(scaler_dir, \"node_scaler.pkl\"))\n",
    "\n",
    "    return train_batch, val_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
