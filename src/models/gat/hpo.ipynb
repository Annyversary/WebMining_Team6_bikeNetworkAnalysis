{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be96ae50",
   "metadata": {},
   "source": [
    "## Hyperparamter Optimization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e900a2",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "This section performs a randomized hyperparameter search within a predefined search space to optimize a `GATv2EdgePredictor` model for edge attribute regression.\n",
    "\n",
    "For each trial, a random configuration is sampled. A model is then instantiated using this configuration. It is trained on the training set for up to `num_epochs`, using the Adam optimizer and Mean Squared Error (MSE) loss. The prediction task targets the specific edge attribute `tracks` (`target_idx = 4`).\n",
    "\n",
    "**Early Stopping:** During training, the model is evaluated on the validation set every 50 epochs. If the validation loss does not improve for a defined number of evaluations (`early_stopping_patience`), training is stopped early to prevent overfitting.\n",
    "\n",
    "**Output:** After all trials are completed, the best-performing model (based on validation loss) and its corresponding hyperparameter configuration are saved:\n",
    "- Model weights: `hpo_models/best_model_overall.pth`\n",
    "- Configuration file: `hpo_models/best_config_overall.json`\n",
    "\n",
    "The `hpo_models/` directory is automatically created if it does not already exist, to store the results. Random seeds are fixed using `random.seed(42)` and `torch.manual_seed(42)` to ensure reproducibility across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceacf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import sys\n",
    "import import_ipynb\n",
    "import joblib\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\",  \"..\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from gatv2 import GATv2EdgePredictor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "def perform_hpo(train_data, val_data, search_space, num_trials=3, num_epochs=1000, early_stopping_patience=1, min_delta=0.01):\n",
    "    \"\"\"\n",
    "    Performs random hyperparameter optimization for a GATv2 edge predictor model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : torch_geometric.data.Data\n",
    "        The training graph data.\n",
    "    val_data : torch_geometric.data.Data\n",
    "        The validation graph data.\n",
    "    search_space : dict\n",
    "        Dictionary specifying possible values for hyperparameters (lr, hidden_channels, etc.).\n",
    "    num_trials : int, optional (default=3)\n",
    "        Number of random hyperparameter configurations to try.\n",
    "    num_epochs : int, optional (default=1000)\n",
    "        Maximum number of training epochs per trial.\n",
    "    early_stopping_patience : int, optional (default=1)\n",
    "        Number of evaluations with no improvement before early stopping.\n",
    "    min_delta : float, optional (default=0.01)\n",
    "        Minimum change in the monitored quantity to qualify as an improvement. \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves the best model and configuration to disk as 'best_model_overall.pth' and 'best_config_overall.json'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Create directory for saving models and configs\n",
    "    os.makedirs(\"hpo_models\", exist_ok=True)\n",
    "\n",
    "    best_overall_val_loss = float(\"inf\")\n",
    "    best_config = None\n",
    "\n",
    "    # Begin hyperparameter search loop\n",
    "    for trial in range(num_trials):\n",
    "        # Randomly sample a configuration from the search space\n",
    "        config = {\n",
    "            \"lr\": random.choice(search_space[\"lr\"]),\n",
    "            \"hidden_channels\": random.choice(search_space[\"hidden_channels\"]),\n",
    "            \"out_channels\": random.choice(search_space[\"out_channels\"]),\n",
    "            \"heads\": random.choice(search_space[\"heads\"]),\n",
    "        }\n",
    "\n",
    "        print(f\"\\n Trial {trial+1}/{num_trials} | Config: {config}\")\n",
    "\n",
    "        # Instantiate model with current configuration\n",
    "        model = GATv2EdgePredictor(\n",
    "            in_channels=train_data.num_node_features,\n",
    "            hidden_channels=config[\"hidden_channels\"],\n",
    "            out_channels=config[\"out_channels\"],\n",
    "            edge_dim=train_data.edge_attr.shape[1],\n",
    "            heads=config[\"heads\"]\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(train_data)\n",
    "            target = train_data.y\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print training loss periodically\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:03d} | Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "            # === Evaluate on validation set every 50 epochs ===\n",
    "            if epoch % 50 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_pred = model(val_data)\n",
    "                    val_target = val_data.y\n",
    "\n",
    "                    # Compute MSE on validation set using scaled values\n",
    "                    val_mse = mean_squared_error(val_target.cpu().numpy(), val_pred.cpu().numpy())\n",
    "\n",
    "                    # Compute RMSE in original scale\n",
    "                    y_scaler = joblib.load(os.path.join(\"..\", \"..\", \"utils\", \"helper_functions\", \"scalers\", \"target_scaler.pkl\"))\n",
    "                    val_pred_orig = y_scaler.inverse_transform(val_pred.cpu().numpy())\n",
    "                    val_target_orig = y_scaler.inverse_transform(val_target.cpu().numpy())\n",
    "                    val_rmse = root_mean_squared_error(val_target_orig, val_pred_orig)\n",
    "\n",
    "                print(f\"Epoch {epoch:03d} | Validation Loss: {val_mse:.4f} || RMSE (original scale): {val_rmse:.2f}\")\n",
    "\n",
    "                # Save the model if it improves the best validation RMSE\n",
    "                if best_val_loss - val_rmse > min_delta:\n",
    "                    best_val_loss = val_rmse\n",
    "                    epochs_without_improvement = 0\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "\n",
    "                # Apply early stopping if no improvement\n",
    "                if epochs_without_improvement >= early_stopping_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Trial {trial+1} complete | Best Validation RMSE: {best_val_loss:.2f}\")\n",
    "\n",
    "        # Update global best model and config if current trial is better\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_config = {\n",
    "                **config,               # original hyperparameters\n",
    "                \"trial\": trial + 1,     # current trial number (1-based)\n",
    "                \"val_loss\": best_val_loss\n",
    "            }\n",
    "            torch.save(model.state_dict(), \"hpo_models/best_model_overall.pth\")\n",
    "            with open(\"hpo_models/best_config_overall.json\", \"w\") as f:\n",
    "                json.dump(best_config, f, indent=2)  # nicely formatted for readability\n",
    "\n",
    "        # Clean up memory\n",
    "        del model, optimizer, pred, target, val_pred, val_target, loss, val_loss\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nRandom search completed.\")\n",
    "    print(f\"Best configuration: {best_config}\")\n",
    "    print(f\"Best validation loss: {best_overall_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947433a4",
   "metadata": {},
   "source": [
    "### Defining the Hyperparameter Search Space and Training Settings\n",
    "\n",
    "The hyperparameter search space is defined as a set of possible values for several key hyperparameters, including:\n",
    "- the learning rate (`lr`),\n",
    "- the number of hidden units in intermediate layers (`hidden_channels`),\n",
    "- the size of the output features (`out_channels`),\n",
    "- and the number of attention heads (`heads`) used in the GATv2 model.\n",
    "\n",
    "This search space can be expanded or modified depending on the model's requirements. For example:\n",
    "- **Optimizer**: We currently use Adam, but alternatives like SGD or AdamW could also be tested.\n",
    "- **Loss function**: MSELoss is used here, but other functions such as L1Loss could be considered.\n",
    "- **Dropout**: This could be applied within GATv2 or in the edge-level MLP, although it is not used in the current implementation.\n",
    "- **Batch size**: Currently, training is performed on the entire graph (no mini-batching). However, support for mini-batching could be implemented.\n",
    "\n",
    "In addition to defining the search space, the training and validation settings are specified as follows:\n",
    "- The number of hyperparameter optimization (HPO) trials (`num_trials`) is set to 20.\n",
    "- Each model is trained for a maximum of 1000 epochs (`num_epochs`).\n",
    "- Early stopping is applied with a patience of X (`early_stopping_patience`), which corresponds to X Ã— 50 training epochs without improvement in the validation loss (since evaluation occurs every 50 epochs).\n",
    "- The minimum change required for a validation loss to be considered an improvement is defined by `min_delta` (set to 0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cef55e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Trial 1/3 | Config: {'lr': 0.001, 'hidden_channels': 8, 'out_channels': 8, 'heads': 2}\n",
      "Epoch 001 | Training Loss: 1.0453\n",
      "Epoch 010 | Training Loss: 1.0133\n",
      "Epoch 020 | Training Loss: 0.9848\n",
      "Epoch 030 | Training Loss: 0.9630\n",
      "Epoch 040 | Training Loss: 0.9473\n",
      "Epoch 050 | Training Loss: 0.9350\n",
      "Epoch 050 | Validation Loss (MSE): 0.9282 || RMSE (original scale): 28.72\n",
      "Epoch 060 | Training Loss: 0.9253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m val_data = torch.load(\u001b[33m\"\u001b[39m\u001b[33m../../../data/data_splits/val_data.pt\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Define number of HPO trials and training settings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mperform_hpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mperform_hpo\u001b[39m\u001b[34m(train_data, val_data, search_space, num_trials, num_epochs, early_stopping_patience, min_delta)\u001b[39m\n\u001b[32m     86\u001b[39m target = train_data.y\n\u001b[32m     87\u001b[39m loss = criterion(pred, target)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m optimizer.step()\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Print training loss periodically\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space (can be extended or modified)\n",
    "search_space = {\n",
    "    \"lr\": [1e-1, 1e-2, 1e-3],                    # Learning rates (optional: 1e-4)\n",
    "    \"hidden_channels\": [8, 16, 32],              # Hidden layer sizes (optional: 64)\n",
    "    \"out_channels\": [8, 16],                     # Output feature sizes from GNN (optional: 32)\n",
    "    \"heads\": [1, 2],                             # Number of attention heads in GATv2 (optional: 4)\n",
    "    # \"dropout\": [0.0, 0.1, 0.3, 0.5],           # Dropout\n",
    "}\n",
    "\n",
    "train_data = torch.load(\"../../../data/data_splits/train_data.pt\", weights_only=False)\n",
    "val_data = torch.load(\"../../../data/data_splits/val_data.pt\", weights_only=False)\n",
    "\n",
    "# Define number of HPO trials and training settings\n",
    "perform_hpo(train_data, val_data, search_space, num_trials=3, num_epochs=1000, early_stopping_patience=3, min_delta = 0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
