{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a458bebc",
   "metadata": {},
   "source": [
    "## Graph Attention Network (GAT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d387ed8",
   "metadata": {},
   "source": [
    "### 1.1 Importing Dependencies\n",
    "\n",
    "We import the necessary libraries and functions, ensuring that all required modules and helper functions are properly integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58fe2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# gat → models → src\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "\n",
    "import import_ipynb \n",
    "from utils.wrapper.transform_networkx_into_pyg import transform_networkx_into_pyg\n",
    "from utils.add_dummy_node_features import add_dummy_node_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98755922",
   "metadata": {},
   "source": [
    "### 1.2 Loading and Preparing Graph Data from GraphML Files\n",
    "\n",
    "This code snippet loads a series of bicycle traffic network graphs stored in GraphML format and prepares them for training with PyTorch Geometric (PyG). The objective is to convert each monthly graph into a format compatible with Graph Neural Networks (GNNs), ensuring that edge features are retained.\n",
    "\n",
    "Each NetworkX graph is converted into a PyG `Data` object using a custom helper function `networkx_to_pyg`. This function ensures that essential edge attributes such as:\n",
    "\n",
    "- `tracks` (the number of bicycles traveling from the starting to the ending point),\n",
    "- `month` and `year`,\n",
    "- `speed_rel` (relative speed),\n",
    "\n",
    "are preserved during the conversion process.\n",
    "\n",
    "PyG expects data in a specific structure, particularly when edge attributes are used in models like GATv2.\n",
    "\n",
    "`data_list` contains multiple `torch_geometric.data.Data` objects, each representing a graph.\n",
    "\n",
    "**NOTE:** So far I only did for data from 2023. I will iterate through all available data, when the implementation is finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731c66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store the PyTorch Geometric Data objects\n",
    "data_list = []\n",
    "\n",
    "# Iterate over the 12 graph files (from 0 to 11)\n",
    "for i in range(12):  # 0 to 11\n",
    "    # Build the path to the graph file\n",
    "    path = f\"../../../graphs/2023/bike_network_2023_{i}.graphml\"\n",
    "    \n",
    "    # Read the graph from the GraphML file\n",
    "    G_nx = nx.read_graphml(path)\n",
    "    \n",
    "    # Ensure the graph is loaded as a directed graph (DiGraph)\n",
    "    G_nx = nx.DiGraph(G_nx)\n",
    "    \n",
    "    # Use the custom function to convert the NetworkX graph to a PyTorch Geometric Data object\n",
    "    # The edge attributes (such as 'tracks', 'month', 'year', 'speed_rel') will be preserved\n",
    "    data = transform_networkx_into_pyg(G_nx)\n",
    "    \n",
    "    # Append the Data object to the list\n",
    "    data_list.append(data)\n",
    "\n",
    "# Check the result - number of graphs (Data objects) loaded\n",
    "print(f\"Number of graphs: {len(data_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d10c2e",
   "metadata": {},
   "source": [
    "### 1.3 Adding Dummy Node Features to the Graphs\n",
    "\n",
    "In this section of the code, we add **dummy node features** to our graphs. This process ensures that each node in our graphs has a **feature dimension**, even if no node features were originally present. This is an important step in preparing the data for use in Graph Neural Networks (GNNs).\n",
    "\n",
    "**NOTE:** At a later stage, once we have implemented feature engineering, we will replace the dummy features with the engineered ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c85152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = add_dummy_node_features(data_list, feature_dim=1, value=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2dd7ab",
   "metadata": {},
   "source": [
    "### 1.4 Train-Validation Split\n",
    "\n",
    "For predicting edge attributes (e.g., `tracks`), an 80/20 train/validation split was applied to the **existing edges within each graph**.\n",
    "\n",
    "In our application, the **nodes represent physically existing bike stations**, which typically do not change or only change very infrequently. The aim of the analysis is to model the **connections between stations**, i.e., to understand and predict how many bicycles move along certain routes (in other words: edges with weights).\n",
    "\n",
    "A **node-level split** (i.e., an 80/20 split of the nodes themselves) would mean that some stations would be completely unseen during training. This would not be meaningful because:\n",
    "\n",
    "- The **stations themselves are not the prediction target**;\n",
    "- It is the **relationships or transitions between the stations (edges)** that should be modeled;\n",
    "- In deployment, **all stations are known** (they are physically installed in the system);\n",
    "\n",
    "**NOTE:** GCN and GCN-GRU could also use these functions. So we would move them to the folder utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf90480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 0:\n",
      "  Train edges: 16671\n",
      "  Val edges:   8334\n",
      "Graph 1:\n",
      "  Train edges: 17980\n",
      "  Val edges:   8988\n",
      "Graph 2:\n",
      "  Train edges: 18066\n",
      "  Val edges:   9032\n",
      "Graph 3:\n",
      "  Train edges: 22253\n",
      "  Val edges:   11126\n",
      "Graph 4:\n",
      "  Train edges: 26821\n",
      "  Val edges:   13410\n",
      "Graph 5:\n",
      "  Train edges: 38612\n",
      "  Val edges:   19304\n",
      "Graph 6:\n",
      "  Train edges: 31458\n",
      "  Val edges:   15728\n",
      "Graph 7:\n",
      "  Train edges: 30864\n",
      "  Val edges:   15432\n",
      "Graph 8:\n",
      "  Train edges: 30796\n",
      "  Val edges:   15396\n",
      "Graph 9:\n",
      "  Train edges: 24759\n",
      "  Val edges:   12378\n",
      "Graph 10:\n",
      "  Train edges: 19477\n",
      "  Val edges:   9738\n",
      "Graph 11:\n",
      "  Train edges: 14930\n",
      "  Val edges:   7464\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "# Define the 80/20 train/val split\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.2,  # 20% for validation\n",
    "    num_test=0.0,  # no test set (test set will be 2024 data)\n",
    "    is_undirected=False,  # Set to False if your graphs are directed\n",
    "    split_labels=False,\n",
    ")\n",
    "\n",
    "# Apply the transform to each graph in your list\n",
    "train_val_data_list = [transform(data) for data in data_list]\n",
    "\n",
    "# Now you get a list of (train_data, val_data) tuples\n",
    "for i, (train_data, val_data, _) in enumerate(train_val_data_list):\n",
    "    print(f\"Graph {i}:\")\n",
    "    print(f\"  Train edges: {train_data.edge_index.size(1)}\")\n",
    "    print(f\"  Val edges:   {val_data.edge_label_index.size(1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1c50d",
   "metadata": {},
   "source": [
    "## 2. Implementing a GAT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd348f6d",
   "metadata": {},
   "source": [
    "### 2.1 From GAT to GATv2\n",
    "\n",
    "We initially started by implementing a vanilla GAT model—an advanced type of Graph Neural Network (GNN) that leverages **attention mechanisms**. However, we soon realized that this model is not capable of learning from **edge attributes**, which is essential for our task. This limitation became especially critical because our original dataset does not contain any **node attributes** at all.\n",
    "\n",
    "This led us to adopt the **GATv2** model, which is specifically designed to aggregate node features while also considering **edge attributes** during message passing. It is more suitable for our purposes.\n",
    "\n",
    "The GATv2 model expects the following **inputs**:\n",
    "- `x`: Node features → `data.x`\n",
    "- `edge_index`: Edge list → `data.edge_index`\n",
    "- `edge_attr`: Edge attributes → `data.edge_attr`\n",
    "\n",
    "These inputs are automatically passed from the `Data` object when calling the model.\n",
    "\n",
    "**Output:**  \n",
    "The model returns **node representations (embeddings)**—a tensor with one row per node and one column per output feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a77b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, edge_dim, heads=1):\n",
    "        super(GATv2, self).__init__()\n",
    "\n",
    "        # First GATv2 layer, with edge attributes\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=heads, edge_dim=edge_dim)\n",
    "\n",
    "        # Second GATv2 layer, output dimension = out_channels\n",
    "        self.gat2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Apply first GATv2 layer with edge attributes\n",
    "        x = self.gat1(x, edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # Apply second GATv2 layer\n",
    "        x = self.gat2(x, edge_index, edge_attr)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517e9d4",
   "metadata": {},
   "source": [
    "### 2.2 Advancing to an Encoder-Decoder Architecture\n",
    "\n",
    "However, the original GATv2 model is primarily designed to learn **node embeddings**. These are useful for tasks such as node classification but are **not directly applicable to predicting edge attributes** like our target edge weight `tracks`.\n",
    "\n",
    "Since our objective is to **predict edge values**, using a node-only model is insufficient. To address this, we extend the GATv2 architecture by incorporating a **decoder module** that transforms node embeddings into edge-level predictions.\n",
    "\n",
    "Our final model follows a typical **encoder-decoder architecture**:\n",
    "\n",
    "- **Encoder:**  \n",
    "  We use the GATv2 model as the encoder to compute **informative node embeddings** based on the graph structure, edge attributes, and (if available) node features.\n",
    "\n",
    "- **Decoder:**  \n",
    "  As the decoder, we use a **small multilayer perceptron (MLP)** that takes as input the **concatenated embeddings** of each edge's source and target nodes.  \n",
    "  This MLP outputs a **single scalar value per edge**, which serves as the prediction for the edge attribute `tracks`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2e4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GATv2EdgePredictor(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 hidden_channels, \n",
    "                 out_channels, \n",
    "                 edge_dim, \n",
    "                 heads=1):\n",
    "        super(GATv2EdgePredictor, self).__init__()\n",
    "\n",
    "        # 1. GATv2 model for computing node embeddings\n",
    "        self.gnn = GATv2(in_channels, hidden_channels, out_channels, edge_dim, heads)\n",
    "\n",
    "        # 2. Edge MLP to predict edge attributes (e.g., \"tracks\")\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels * 2, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, 1)  # Output: a single scalar per edge\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: PyTorch Geometric Data object with attributes:\n",
    "                  - x: node features\n",
    "                  - edge_index: edge connectivity (COO format)\n",
    "                  - edge_attr: edge attributes\n",
    "\n",
    "        Returns:\n",
    "            pred: Tensor of shape [num_edges, 1] with predicted edge weights (e.g., \"tracks\")\n",
    "        \"\"\"\n",
    "        # Compute node embeddings using the GATv2 model\n",
    "        x = self.gnn(data.x, data.edge_index, data.edge_attr)  # [num_nodes, out_channels]\n",
    "\n",
    "        # Construct edge representations by concatenating source and target node embeddings\n",
    "        row, col = data.edge_index  # source & target node indices for each edge\n",
    "        edge_inputs = torch.cat([x[row], x[col]], dim=1)  # [num_edges, out_channels * 2]\n",
    "\n",
    "        # Predict edge weights\n",
    "        pred = self.edge_mlp(edge_inputs)  # [num_edges, 1]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550eede5",
   "metadata": {},
   "source": [
    "## 3. Hyperparamter Optimization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45d136",
   "metadata": {},
   "source": [
    "### 3.1 Setting Random Seeds and Creating Directory for Models and Configurations\n",
    "\n",
    "To ensure the reproducibility of the experiments, random seeds are set for both PyTorch and Python (`torch.manual_seed(42)` and `random.seed(42)`). This guarantees that the results remain consistent across different runs of the code. Additionally, a directory named `hpo_models` is created where models and configuration files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7475f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Create directory for saving models and configs\n",
    "os.makedirs(\"hpo_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04215354",
   "metadata": {},
   "source": [
    "### 3.2 Defining the Hyperparameter Search Space and Training Settings\n",
    "\n",
    "The hyperparameter search space is defined as a set of possible values for several key hyperparameters, including:\n",
    "- the learning rate (`lr`),\n",
    "- the number of hidden units in intermediate layers (`hidden_channels`),\n",
    "- the size of the output features (`out_channels`),\n",
    "- and the number of attention heads (`heads`) used in the GATv2 model.\n",
    "\n",
    "This search space can be expanded or modified depending on the model's requirements. For example:\n",
    "- **Optimizer**: We currently use Adam, but alternatives like SGD or AdamW could also be tested.\n",
    "- **Loss function**: MSELoss is used here, but other functions such as L1Loss could be considered.\n",
    "- **Dropout**: This could be applied within GATv2 or in the edge-level MLP, although it is not used in the current implementation.\n",
    "- **Batch size**: Currently, training is performed on the entire graph (no mini-batching). However, support for mini-batching could be implemented.\n",
    "\n",
    "In addition to defining the search space, the training and validation settings are specified as follows:\n",
    "- The number of hyperparameter optimization (HPO) trials (`num_trials`) is set to 20.\n",
    "- Each model is trained for a maximum of 1000 epochs (`num_epochs`).\n",
    "- Early stopping is applied with a patience of 5 epochs (`early_stopping_patience`), meaning training halts if the validation loss does not improve over five consecutive epochs.\n",
    "\n",
    "The best-performing configuration and the lowest validation loss encountered across all trials are saved for later use. Lastly, the index of the target edge attribute to be predicted (e.g., `\"tracks\"`) is explicitly set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e168918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space (can be extended or modified)\n",
    "search_space = {\n",
    "    \"lr\": [1e-1, 5e-2, 1e-2, 1e-3, 5e-4, 1e-4],  # Learning rates\n",
    "    \"hidden_channels\": [8, 16, 32, 64],          # Hidden layer sizes\n",
    "    \"out_channels\": [8, 16, 32],                 # Output feature sizes from GNN\n",
    "    \"heads\": [1, 2, 4],                          # Number of attention heads in GATv2\n",
    "    # \"dropout\": [0.0, 0.1, 0.3, 0.5],           # Dropout\n",
    "}\n",
    "\n",
    "# Define number of HPO trials and training settings\n",
    "num_trials = 20\n",
    "num_epochs = 1000\n",
    "early_stopping_patience = 5  # Number of epochs to wait for improvement\n",
    "best_overall_val_loss = float(\"inf\")\n",
    "best_config = None\n",
    "\n",
    "# Index of the target edge attribute to predict (e.g., \"tracks\" at column 4)\n",
    "target_idx = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b35df",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 Hyperparameter Optimization with Random Search\n",
    "\n",
    "This section performs a randomized hyperparameter search across a predefined space. For each trial, a set of hyperparameters is sampled and used to instantiate a new `GATv2EdgePredictor` model. The model is trained using the Adam optimizer and Mean Squared Error (MSE) loss for a maximum of `num_epochs` iterations. At the end of all trials, the best overall model and its corresponding configuration are saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf564348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Trial 1/20 | Config: {'lr': 0.0001, 'hidden_channels': 8, 'out_channels': 8, 'heads': 4}\n",
      "Epoch 001 | Training Loss: 308.1389\n",
      "Epoch 010 | Training Loss: 307.6332\n",
      "Epoch 020 | Training Loss: 307.0688\n",
      "Epoch 030 | Training Loss: 306.5000\n",
      "Epoch 040 | Training Loss: 305.9235\n",
      "Epoch 050 | Training Loss: 305.3930\n",
      "Epoch 050 | Validation Loss: 305.3380\n",
      "Epoch 060 | Training Loss: 304.8385\n",
      "Epoch 070 | Training Loss: 304.2743\n",
      "Epoch 080 | Training Loss: 303.6881\n",
      "Epoch 090 | Training Loss: 303.0814\n",
      "Epoch 100 | Training Loss: 302.4483\n",
      "Epoch 100 | Validation Loss: 302.3837\n",
      "Epoch 110 | Training Loss: 301.7864\n",
      "Epoch 120 | Training Loss: 301.0919\n",
      "Epoch 130 | Training Loss: 300.3627\n",
      "Epoch 140 | Training Loss: 299.5991\n",
      "Epoch 150 | Training Loss: 298.7982\n",
      "Epoch 150 | Validation Loss: 298.7162\n",
      "Epoch 160 | Training Loss: 297.9598\n",
      "Epoch 170 | Training Loss: 297.1698\n",
      "Epoch 180 | Training Loss: 296.3419\n",
      "Epoch 190 | Training Loss: 295.4757\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m target = train_data.edge_attr[:, target_idx].unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     36\u001b[39m loss = criterion(pred, target)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m optimizer.step()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Print training loss periodically\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Begin hyperparameter search loop\n",
    "for trial in range(num_trials):\n",
    "    # Randomly sample a configuration from the search space\n",
    "    config = {\n",
    "        \"lr\": random.choice(search_space[\"lr\"]),\n",
    "        \"hidden_channels\": random.choice(search_space[\"hidden_channels\"]),\n",
    "        \"out_channels\": random.choice(search_space[\"out_channels\"]),\n",
    "        \"heads\": random.choice(search_space[\"heads\"]),\n",
    "        # \"dropout\": random.choice(search_space[\"dropout\"]),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n Trial {trial+1}/{num_trials} | Config: {config}\")\n",
    "\n",
    "    # Instantiate model with current configuration\n",
    "    model = GATv2EdgePredictor(\n",
    "        in_channels=train_data.num_node_features,\n",
    "        hidden_channels=config[\"hidden_channels\"],\n",
    "        out_channels=config[\"out_channels\"],\n",
    "        edge_dim=train_data.edge_attr.shape[1],\n",
    "        heads=config[\"heads\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(train_data)\n",
    "        target = train_data.edge_attr[:, target_idx].unsqueeze(1)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training loss periodically\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d} | Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on validation set every 50 epochs\n",
    "        if epoch % 50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(val_data)\n",
    "                val_target = val_data.edge_attr[:, target_idx].unsqueeze(1)\n",
    "                val_loss = criterion(val_pred, val_target)\n",
    "            print(f\"Epoch {epoch:03d} | Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "            # Save the model if it improves the best validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            # Apply early stopping if no improvement\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Trial {trial+1} complete | Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Update global best model and config if current trial is better\n",
    "    if best_val_loss < best_overall_val_loss:\n",
    "        best_overall_val_loss = best_val_loss\n",
    "        best_config = {\n",
    "            **config,               # original hyperparameters\n",
    "            \"trial\": trial + 1,     # current trial number (1-based)\n",
    "            \"val_loss\": best_val_loss.item()  # best validation loss\n",
    "        }\n",
    "        torch.save(model.state_dict(), \"hpo_models/best_model_overall.pth\")\n",
    "        with open(\"hpo_models/best_config_overall.json\", \"w\") as f:\n",
    "            json.dump(best_config, f, indent=2)  # nicely formatted for readability\n",
    "\n",
    "\n",
    "print(\"\\nRandom search completed.\")\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "print(f\"Best validation loss: {best_overall_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
